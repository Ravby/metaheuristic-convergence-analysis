# CEC Competition Confidence Bands based on Rating

This project aims to provide tools for comparing evolutionary algorithms with the winners of the CEC (Congress on Evolutionary Computation) 2017, 2022, and 2024 bound constraint competitions as well as CEC 2024 constraint multi-objective competition.
The project utilizes the EARS (Evolutionary Algorithm Rating System) framework to generate rating intervals and confidence bands based on rating.

## Usage

### Dependencies

This project relies on the EARS (Evolutionary Algorithm Rating System) framework, available at [EARS GitHub Repository](https://github.com/UM-LPM/EARS).

### Project Structure

- **results_files**: Contains official result files obtained from the following sources:
    - [CEC2017](https://github.com/P-N-Suganthan/CEC2017-BoundContrained)
    - [CEC2022](https://github.com/P-N-Suganthan/2022-SO-BO)
    - [CEC2024](https://github.com/P-N-Suganthan/2024-CEC)
    - [CEC2024CMOP](https://github.com/P-N-Suganthan/2024-CEC)

- **documentation**: Contains the official technical reports and results analysis for each competition.

- **experimental_results**: Contains the results generated by EARS (rating intervals and confidence bands).

- **Benchmark Classes**: For each CEC competition, there is a benchmark class (e.g., [Cec2017StoredBenchmark](src/main/java/si/um/feri/lpm/Cec2017StoredBenchmark.java)) with specific information about the competition and selected problems.

- **Run Classes**: These classes (e.g., [RunCec2017](src/main/java/si/um/feri/lpm/RunCec2017.java)) execute the benchmark using a list of algorithms for comparison. By default, results are stored in the [experimental_results](experimental_results) folder.

- **GenerateCec2024Results** is a class that generates the results for the CEC 2024 competition for the selected algorithms included in the EARS framework.

### Usage Instructions

1. **Format Result Files**: Ensure that the result files follow the official guidelines provided by each competition's "Problem Definitions and Evaluation Criteria" PDFs located in the [documentation](documentation) directory.
2. **Add Result Files**: Add your algorithm's result files to the official results folder ([results_files](results_files)) based on the competition.
3. **Run Benchmark**: Add your algorithm to the players list (`players.add(new DummyAlgorithm("AlgorithmName", algorithmResultsDir, fileFormat));`) in one of the run classes to execute the benchmark, comparing your algorithm with CEC winners or any other algorithm included in the benchmark.
4. **Plot Rating bands**: Run the kotlin notebook e.g., [PlotCec2017RatingBands](src/main/kotlin/si/um/feri/lpm/PlotCec2017RatingBands.kt) to plot the rating bands for the selected competition.